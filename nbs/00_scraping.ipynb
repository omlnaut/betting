{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev import *\n",
    "# default_exp scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping\n",
    "> A lot of the useful datasets are not in a ready-to-download format. Instead, they have to be collected over a variety of sub-pages. The following methods are utility for dealing with those kind of situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_link = r'https://www.football-data.co.uk/data.php'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_html(url, encoding='utf-8', bs=True):\n",
    "    'Get the html code for a given url. If bs=True (which is the default), return the parsed BeautifulSoup object instead.'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    html = response.read().decode(encoding=encoding)\n",
    "    if bs: return BeautifulSoup(html, features=\"lxml\")\n",
    "    else: return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<HTML>\\n\\n<H', '\\n</HTML>\\n\\n')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_html = get_html(example_link, bs=False)\n",
    "\n",
    "example_html[:10], example_html[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "CACHE_DIR = Path('../data/cache')\n",
    "def cache(url, cache_name):\n",
    "    chache_path = CACHE_DIR/cache_name\n",
    "    if chache_path.is_file():\n",
    "        bs = BeautifulSoup(chache_path.open(encoding='utf-8'), features=\"lxml\")\n",
    "    else:\n",
    "        bs = get_html(url)\n",
    "        with chache_path.open('w', encoding='utf-8') as f:\n",
    "            f.write(str(bs))\n",
    "        \n",
    "    return bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing purposes we should reduce the amount of actual http traffic, so we'll cache sites that are only used for testing the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chache_html = cache(example_link, 'cache_example')\n",
    "\n",
    "assert (CACHE_DIR/'cache_example').is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def find_links_by_func(html, func=None, return_href=True):\n",
    "    \"\"\"Iterate over all links of the given html-BeautifulSoup-object.\n",
    "    Return a list of all links for which func returns True.\n",
    "    If no func is given, return all links\n",
    "    If return_href=False, return a list of BeautifulSoup link objects\"\"\"\n",
    "    if func is None:\n",
    "        func = lambda target: True\n",
    "    \n",
    "    links = []\n",
    "    for link in html.find_all('a'):\n",
    "        target = link.get('href')\n",
    "        if target:\n",
    "            if func(target):\n",
    "                if return_href: links.append(target)\n",
    "                else: links.append(link)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All links: 209\n"
     ]
    }
   ],
   "source": [
    "example_bs = BeautifulSoup(example_html)\n",
    "\n",
    "all_links = find_links_by_func(example_bs)\n",
    "print(f'All links: {len(all_links)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute links in site: 102\n"
     ]
    }
   ],
   "source": [
    "absolute_in_site_links = find_links_by_func(example_bs, lambda link: 'www.football-data.co.uk' in link)\n",
    "print(f'Absolute links in site: {len(absolute_in_site_links)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return_href=True: https://www.football-data.co.uk/\n",
      "return_href=False: <a href=\"https://www.football-data.co.uk/\"><img alt=\"Football Betting - Football Results - Free Bets\" border=\"0\" src=\"https://www.football-data.co.uk/logo2.jpg\"/></a>\n"
     ]
    }
   ],
   "source": [
    "print(f'return_href=True: {find_links_by_func(example_bs)[0]}')\n",
    "print(f'return_href=False: {find_links_by_func(example_bs, return_href=False)[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def find_links_by_pattern(html, pattern, return_href=True):\n",
    "    \"\"\"Iterate over all links of the given html-BeautifulSoup-object.\n",
    "    Return a list of all links that match the given (regex)pattern.\n",
    "    Patterns passed as string will be compiled to regex.\"\"\"\n",
    "    if isinstance(pattern, str):\n",
    "        pattern = re.compile(pattern)\n",
    "    return find_links_by_func(html, func=lambda target: pattern.match(target), return_href=return_href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Links containg at least one number: 28\n"
     ]
    }
   ],
   "source": [
    "number_links = find_links_by_pattern(example_bs, r'.+\\d+.+')\n",
    "print(f'Links containg at least one number: {len(number_links)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
